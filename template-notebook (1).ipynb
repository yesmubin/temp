{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Statements**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch\nfrom torch import nn \nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\nimport os\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:05:05.569737Z","iopub.execute_input":"2023-09-01T18:05:05.570109Z","iopub.status.idle":"2023-09-01T18:05:09.098878Z","shell.execute_reply.started":"2023-09-01T18:05:05.570057Z","shell.execute_reply":"2023-09-01T18:05:09.097939Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# # **Get Dataset**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # **#Create Datasets and DataLoaders (data_setup.py)**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nContains functionality for creating PyTorch DataLoaders for \nimage classification data.\n\"\"\"\nimport os\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nNUM_WORKERS = os.cpu_count()\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    transform: transforms.Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n  \"\"\"Creates training and testing DataLoaders.\n\n  Takes in a training directory and testing directory path and turns\n  them into PyTorch Datasets and then into PyTorch DataLoaders.\n\n  Args:\n    train_dir: Path to training directory.\n    test_dir: Path to testing directory.\n    transform: torchvision transforms to perform on training and testing data.\n    batch_size: Number of samples per batch in each of the DataLoaders.\n    num_workers: An integer for number of workers per DataLoader.\n\n  Returns:\n    A tuple of (train_dataloader, test_dataloader, class_names).\n    Where class_names is a list of the target classes.\n    Example usage:\n      train_dataloader, test_dataloader, class_names = \\\n        = create_dataloaders(train_dir=path/to/train_dir,\n                             test_dir=path/to/test_dir,\n                             transform=some_transform,\n                             batch_size=32,\n                             num_workers=4)\n  \"\"\"\n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:05:16.531186Z","iopub.execute_input":"2023-09-01T18:05:16.531551Z","iopub.status.idle":"2023-09-01T18:05:16.540942Z","shell.execute_reply.started":"2023-09-01T18:05:16.531521Z","shell.execute_reply":"2023-09-01T18:05:16.539617Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# # **Making a model**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn \n\nclass TinyVGG(nn.Module):\n  \"\"\"Creates the TinyVGG architecture.\n\n  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n\n  Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n  \"\"\"\n  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n      super().__init__()\n      self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, \n                    stride=1, \n                    padding=0),  \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2)\n      )\n      self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n      )\n      self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n      )\n\n  def forward(self, x: torch.Tensor):\n      x = self.conv_block_1(x)\n      x = self.conv_block_2(x)\n      x = self.classifier(x)\n      return x\n      # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:05:19.471030Z","iopub.execute_input":"2023-09-01T18:05:19.471434Z","iopub.status.idle":"2023-09-01T18:05:19.482834Z","shell.execute_reply.started":"2023-09-01T18:05:19.471404Z","shell.execute_reply":"2023-09-01T18:05:19.481873Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Creating train_step() and test_step() functions and train() to combine them**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n  \"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n  \"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List]:\n  \"\"\"Trains and tests a PyTorch model.\n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results,model","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:05:22.636864Z","iopub.execute_input":"2023-09-01T18:05:22.637287Z","iopub.status.idle":"2023-09-01T18:05:22.659443Z","shell.execute_reply.started":"2023-09-01T18:05:22.637251Z","shell.execute_reply":"2023-09-01T18:05:22.658373Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Save Model**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nContains various utility functions for PyTorch model training and saving.\n\"\"\"\nimport torch\nfrom pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n  \"\"\"Saves a PyTorch model to a target directory.\n\n  Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n\n  Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n  \"\"\"\n  # Create target directory\n  target_dir_path = Path(target_dir)\n  target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n\n  # Create model save path\n  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n  model_save_path = target_dir_path / model_name\n\n  # Save the model state_dict()\n  print(f\"[INFO] Saving model to: {model_save_path}\")\n  torch.save(obj=model.state_dict(),\n             f=model_save_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:05:28.124462Z","iopub.execute_input":"2023-09-01T18:05:28.124828Z","iopub.status.idle":"2023-09-01T18:05:28.133161Z","shell.execute_reply.started":"2023-09-01T18:05:28.124797Z","shell.execute_reply":"2023-09-01T18:05:28.131017Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTrains a PyTorch image classification model using device-agnostic code.\n\"\"\"\n\nimport os\nimport torch\n\n\nfrom torchvision import transforms\n\n# Setup hyperparameters\nNUM_EPOCHS = 5\nBATCH_SIZE = 32\nHIDDEN_UNITS = 10\nLEARNING_RATE = 0.001\n\n# Setup directories\ntrain_dir = \"/kaggle/input/labeled-chest-xray-images/chest_xray/train\"\ntest_dir = \"/kaggle/input/labeled-chest-xray-images/chest_xray/train\"\n\n# Setup target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create transforms\ndata_transform = transforms.Compose([\n  transforms.Resize((64, 64)),\n  transforms.ToTensor()\n])\n\n# Create DataLoaders with help from data_setup.py\ntrain_dataloader, test_dataloader, class_names = create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=data_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create model with help from model_builder.py\nmodel = TinyVGG(\n    input_shape=3,\n    hidden_units=HIDDEN_UNITS,\n    output_shape=len(class_names)\n).to(device)\n\n# Set loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Start training with help from engine.py\nresults,model=train(model=model,\n             train_dataloader=train_dataloader,\n             test_dataloader=test_dataloader,\n             loss_fn=loss_fn,\n             optimizer=optimizer,\n             epochs=NUM_EPOCHS,\n             device=device)\n\n# Save the model with help from utils.py\nsave_model(model=model,\n                 target_dir=\"models\",\n                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:05:31.964614Z","iopub.execute_input":"2023-09-01T18:05:31.964977Z","iopub.status.idle":"2023-09-01T18:16:54.118307Z","shell.execute_reply.started":"2023-09-01T18:05:31.964945Z","shell.execute_reply":"2023-09-01T18:16:54.117185Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35fd1d6b202e46dfa37b10d035779e5d"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 0.5605 | train_acc: 0.7450 | test_loss: 0.3563 | test_acc: 0.8548\nEpoch: 2 | train_loss: 0.2270 | train_acc: 0.9089 | test_loss: 0.1711 | test_acc: 0.9304\nEpoch: 3 | train_loss: 0.1631 | train_acc: 0.9381 | test_loss: 0.1502 | test_acc: 0.9449\nEpoch: 4 | train_loss: 0.1489 | train_acc: 0.9434 | test_loss: 0.1466 | test_acc: 0.9451\nEpoch: 5 | train_loss: 0.1375 | train_acc: 0.9476 | test_loss: 0.1339 | test_acc: 0.9480\n[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"model.parameters","metadata":{"execution":{"iopub.status.busy":"2023-09-01T18:20:56.770234Z","iopub.execute_input":"2023-09-01T18:20:56.770624Z","iopub.status.idle":"2023-09-01T18:20:56.778019Z","shell.execute_reply.started":"2023-09-01T18:20:56.770595Z","shell.execute_reply":"2023-09-01T18:20:56.776920Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<bound method Module.parameters of TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=1690, out_features=2, bias=True)\n  )\n)>"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}